---
layout: default
title: Matthew Nokleby
---

I am currently Lead AI Scientist at the Target Corporation, Minneapolis, MN.

Previously, I was an assistant professor in the ECE department at Wayne State University. I received my Ph.D. from Rice University, where I worked at the Center for Multimedia Communication.

I'm interested in all things data and probability: machine/deep learning, computer vision, (stochastic, distributed) optimization, and information theory.

[[resume](resume.pdf)] &nbsp;&nbsp;&nbsp;&nbsp; [[curriculum vitae](cv.pdf)] &nbsp;&nbsp;&nbsp;&nbsp; [[github repo](https://github.com/docnok)] &nbsp;&nbsp;&nbsp;&nbsp; [[publication list](publications.html)]

---

### Recent news:
- Gave a tutorial talk, "An Introduction to Deep Metric Learning", for Data Science Minneapolis in July 2019. Slides available [here](talks/dsm-jul-2019/#).
- June 2019: Presented "An Effective Label Noise Modelfor DNN Text Classification" at NAACL 2019 in Minneapolis. Joint work with former PhD student Ishan Jindal and collaborators Daniel Pressel and Brian Lester.
- May 2019: New work "A Nonlinear, Noise-aware, Quasi-clustering Approach to Learning Deep CNNs from Noisy Labels" accepted to the CVPR Workshop on Uncertainty and Robustness in Deep Learning. Work with Ishan Jindal, Daniel Pressel, and Xuwen Chen.
- May 2019: Presented "Anytime Minibatch: Exploiting Stragglers in Online Distributed Optimization" at ICLR 2019 in New Orleans. Joint work with Nuwan Ferdinand, Stark Draper, and Haider Al-Lawati at the University of Toronto.
- Mar. 2019: Journal paper "Stochastic Optimization from Distributed, Streaming Data in Rate-limited Networks" appears in IEEE Transactions on Signal and Information Processing over Networks. Joint work with Waheed U. Bajwa at Rutgers.
